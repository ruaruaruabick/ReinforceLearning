{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本代码实现了如下功能：\n",
    "- 环境为CartPole-v1，详见https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "- 原始Q学习，Q学习+目标网络，双Q学习\n",
    "- 对决网络\n",
    "- 有限经验回放\n",
    "- 加噪网络\n",
    "- 改进的奖励计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import sys\n",
    "\n",
    "#定义网络\n",
    "\"\"\"\n",
    "输入：\n",
    "1. Cart Position: -4.8-4.8, 超过2.4时小车会停止运动\n",
    "2. Cart Velocity：-Inf-Inf, \n",
    "3. Pole Angle：-.418, .418\n",
    "4. Pole Angular Velocity：-Inf-Inf, \n",
    "\"\"\"\n",
    "#加噪网络\n",
    "class NoiseLinear(nn.Linear):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__(in_features, out_features)\n",
    "        self.p_noise_weight = nn.Parameter(torch.full(self.weight.size(), 0.))\n",
    "        self.p_noise_bias = nn.Parameter(torch.full(self.bias.size(), 0.))\n",
    "    def forward(self, x):\n",
    "        weight_noise = torch.randn_like(self.p_noise_weight,device=x.device)\n",
    "        bias_noise = torch.randn_like(self.p_noise_bias,device=x.device)\n",
    "        out = super().forward(x)\n",
    "        if self.training:\n",
    "            out = out + torch.mm(x,(self.p_noise_weight * weight_noise).T) + self.p_noise_bias * bias_noise\n",
    "        return out\n",
    "\n",
    "#原始 DQN\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim = 128, output_dim = 2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(NoiseLinear(input_dim,hidden_dim),nn.LeakyReLU(),\n",
    "                                   NoiseLinear(hidden_dim,output_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class DuelingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim = 128, output_dim = 2):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(NoiseLinear(input_dim,hidden_dim),nn.LeakyReLU(),\n",
    "                                   NoiseLinear(hidden_dim,hidden_dim//2),nn.LeakyReLU())\n",
    "        self.v_head = nn.Sequential(NoiseLinear(hidden_dim//2,hidden_dim//4),nn.LeakyReLU(),NoiseLinear(hidden_dim//4,1))#近似V*\n",
    "        self.d_head = nn.Sequential(NoiseLinear(hidden_dim//2,hidden_dim//4),nn.LeakyReLU(),NoiseLinear(hidden_dim//4,output_dim))#近似D*\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden = self.model(x)\n",
    "        v = self.v_head(hidden)\n",
    "        d = self.d_head(hidden)\n",
    "        q = v + d - d.mean(dim=1,keepdim=True) #采用mean而不是max实现\n",
    "        return q\n",
    "    \n",
    "#定义Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, device, buffer_size, game_name, fea_dim, hidden_dim, action_dim, gamma = 0.98):\n",
    "        self.model = DuelingNetwork(fea_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_model = DuelingNetwork(fea_dim, hidden_dim, action_dim).to(device) # 为保证TD目标尽量不变， 新建目标网络，更新较慢\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.requires_grad_(False) # 目标网络不需要梯度\n",
    "        self.replay_buffer = []#(s_t:,a_t,r_t,s_t+1)\n",
    "        self.replay_buffer_weight = [] # 权重\n",
    "        self.action_dim = action_dim\n",
    "        self.fea_dim = fea_dim\n",
    "        self.game_name = game_name\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.env = gym.make(self.game_name)\n",
    "        \n",
    "        self.buffer_size = buffer_size\n",
    "        self.device = device\n",
    "        self.gamma = gamma # 回报率\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def reset(self):\n",
    "        observation, info = self.env.reset()\n",
    "        observation = torch.from_numpy(observation).unsqueeze(0)\n",
    "        return observation\n",
    "    \n",
    "    def take_action(self, state: torch.Tensor | np.ndarray, epsilon: float, verbose = False) -> torch.Tensor:\n",
    "        bs = 1\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.from_numpy(state).unsqueeze(0)\n",
    "        state = state.to(self.device)\n",
    "        # epsilon-greedy策略\n",
    "        if random.random() < epsilon:\n",
    "            # 随机动作\n",
    "            action = torch.randint(0,self.action_dim,(bs,))\n",
    "        else:\n",
    "            #最优动作价值函数\n",
    "            with torch.no_grad():\n",
    "                Q = self.model(state)\n",
    "            action = torch.argmax(Q,dim=1)\n",
    "            if verbose:\n",
    "                print(f\"Q:{Q[:,action].item():.2f}\")\n",
    "        return action\n",
    "    \n",
    "    def replay(self, state: torch.Tensor, epsilon: float)->bool:\n",
    "        action = self.take_action(state, epsilon)\n",
    "        next_state, reward, terminated,truncated, _ = self.env.step(action.item()) #\n",
    "        next_state = torch.from_numpy(next_state).unsqueeze(0)\n",
    "        next_state = next_state\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        done = torch.tensor([done],dtype=torch.bool)\n",
    "        \n",
    "        reward = self.compute_reward(next_state, done)\n",
    "        reward = torch.tensor([reward])\n",
    "        \n",
    "        #弹出不好的的策略\n",
    "        if len(self.replay_buffer) > self.buffer_size:\n",
    "            self.replay_buffer.pop(0)\n",
    "            self.replay_buffer_weight.pop(0)\n",
    "        self.replay_buffer.append((state.cpu(), action.cpu(), reward.cpu(), next_state.cpu(), done.cpu())) \n",
    "        self.replay_buffer_weight.append(1.0)#默认权重1.0\n",
    "        return next_state, action.item(), reward.item(), done.item()\n",
    "    \n",
    "    def sample_data(self, batch_size=32):\n",
    "        #随机采样batch_size个数据\n",
    "        assert len(self.replay_buffer) == len(self.replay_buffer_weight)\n",
    "        total_weight = sum(self.replay_buffer_weight)\n",
    "        sample_prob = [item/total_weight for item in self.replay_buffer_weight]   # 采样概率\n",
    "        \n",
    "        indexs = np.random.choice(range(len(self.replay_buffer)), batch_size,replace=False,p=sample_prob)#不放回采样\n",
    "        indexs = indexs.tolist()\n",
    "        batch = [self.replay_buffer[i] for i in indexs]\n",
    "        state, action, reward, next_state, done = map(torch.cat, zip(*batch))\n",
    "        state = state.to(self.device)\n",
    "        action = action.to(self.device)\n",
    "        reward = reward.to(self.device)\n",
    "        next_state = next_state.to(self.device)\n",
    "        done = done.to(self.device)\n",
    "        \n",
    "        return state, action, reward, next_state, done, indexs\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def test(self):\n",
    "        self.model.eval()\n",
    "        self.target_model.eval()\n",
    "        env_test = gym.make(self.game_name,render_mode = 'human')\n",
    "        state, _ = env_test.reset()\n",
    "        done = False\n",
    "        step = 0 \n",
    "        input('TESTING, press enter to continue')\n",
    "        while not done:\n",
    "            action = self.take_action(state, epsilon=0, verbose=True)\n",
    "            next_state, reward, terminated,truncated, _ = env_test.step(action.item())\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            step += 1\n",
    "            time.sleep(0.01)\n",
    "        print(f\"Running {step} steps, last_state:{state}\")\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def valid(self, ):\n",
    "        self.model.eval()\n",
    "        env_test = gym.make(self.game_name)\n",
    "        result = 0\n",
    "        for i in range(5):\n",
    "            state, _ = env_test.reset()\n",
    "            done = False\n",
    "            step = 0 \n",
    "            total_reward = 0\n",
    "            while not done:\n",
    "                action = self.take_action(state, epsilon=0)\n",
    "                next_state, reward, terminated,truncated, _ = env_test.step(action.item())\n",
    "                done = terminated or truncated\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                step += 1\n",
    "            result += total_reward\n",
    "        self.model.train()\n",
    "        return result/5\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_target_model(self, decay = 0.99):\n",
    "        for param, target_param in zip(self.model.parameters(), self.target_model.parameters()):\n",
    "            newparam = decay * target_param.data + (1 - decay) * param.data\n",
    "            target_param.data.copy_(newparam)\n",
    "            \n",
    "    def compute_reward(self, state, done):\n",
    "        x, x_dot, theta, theta_dot = state[0]\n",
    "        angle_penalty = theta ** 2  # 杆子角度惩罚（直立时θ≈0）\n",
    "        position_penalty = 0.1 * x ** 2  # 小车位置惩罚（中心时x=0）\n",
    "        if done:\n",
    "            return -10.0  # 倒下强惩罚\n",
    "        return 1.0 - angle_penalty - position_penalty  # 奖励函数`\n",
    "    \n",
    "def compute_epsilon(cur_value, decay = 0.999, min_value= 0.01):\n",
    "    if cur_value < min_value:\n",
    "        return min_value\n",
    "    else:\n",
    "        return decay * cur_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607c19bcc58d4d9cb40061b5a7c42896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "writer = SummaryWriter()    \n",
    "lr = 1e-4\n",
    "batch_size = 128\n",
    "replay_step = 100\n",
    "buffer_size = 10000\n",
    "num_round = 2000\n",
    "epsilon = 1\n",
    "gamma = 0.9\n",
    "device = 'cuda' \n",
    "game_name = 'CartPole-v1'\n",
    "fea_dim = 4\n",
    "hidden_dim = 128\n",
    "action_dim = 2\n",
    "agent = DQNAgent(device, buffer_size,game_name,fea_dim,hidden_dim,action_dim,gamma)\n",
    "optimizer = torch.optim.AdamW(agent.model.parameters(), lr=lr)\n",
    "tqdm_bar = tqdm(total=num_round)\n",
    "train_step = 0\n",
    "for cur_round in range(num_round):\n",
    "    epsilon = compute_epsilon(epsilon)\n",
    "    cur_time = 0\n",
    "    cur_reward = 0\n",
    "    state = agent.reset()\n",
    "    done = False\n",
    "    losses = []\n",
    "    while not done:\n",
    "        next_state, action, reward, done = agent.replay(state, epsilon=epsilon)\n",
    "        if done:\n",
    "            state = agent.reset()\n",
    "        #有足够量数据才开始训练\n",
    "        if cur_round > replay_step:\n",
    "            #training\n",
    "            b_state, b_action, b_reward, b_next_state, b_done, indexs = agent.sample_data(batch_size)\n",
    "            q_values = agent.model(b_state).gather(1, b_action[:,None]).squeeze(1)\n",
    "            #原始Q学习，Q学习+目标网络，双Q学习\n",
    "            #选择动作和计算Q_next都用DQN——原始Q学习\n",
    "            #选择动作和计算Q_next都用目标网络——Q学习+目标网络\n",
    "            #选择动作用DQN，计算Q_next用目标网络——双Q学习,效果最好，本代码实现方式\n",
    "            #compute\n",
    "            q_values_next = agent.target_model(b_next_state)\n",
    "            #choose\n",
    "            with torch.no_grad():\n",
    "                next_action = agent.model(b_next_state).max(1)[1]\n",
    "                \n",
    "            q_values_next_max = q_values_next.gather(1, next_action[:,None]).squeeze(1)\n",
    "            td_target = b_reward + q_values_next_max * agent.gamma\n",
    "            loss = agent.loss(q_values, td_target)# 如果td_target为常数，应该detach，但torch自动求导，所以不detach\n",
    "            #更新采样权重\n",
    "            weight = torch.clip((q_values - td_target).abs(), 1e-2).tolist()\n",
    "            for i,w in zip(indexs,weight):\n",
    "                agent.replay_buffer_weight[i] = w\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.model.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            train_step += 1\n",
    "            losses.append(loss.item())\n",
    "            agent.update_target_model()\n",
    "        cur_reward += reward\n",
    "        cur_time += 1\n",
    "        state = next_state\n",
    "    \n",
    "    # valid\n",
    "    valid_reward = agent.valid()\n",
    "    \n",
    "    loss = np.mean(losses) if len(losses) > 0 else 0\n",
    "    writer.add_scalar('train/loss', loss, cur_round)\n",
    "    writer.add_scalar('train/reward', cur_reward, cur_round)\n",
    "    writer.add_scalar('train/epsilon', epsilon, cur_round)\n",
    "    writer.add_scalar('valid/reward', valid_reward, cur_round)\n",
    "    tqdm_bar.set_postfix_str(f'Round {cur_round}/{num_round}, Reward {cur_reward}, Time {cur_time}, loss {loss}, traing step {train_step}')\n",
    "    tqdm_bar.update(1)\n",
    "    if valid_reward == 500:\n",
    "        break\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:10.96\n",
      "Q:10.71\n",
      "Q:11.09\n",
      "Q:10.65\n",
      "Q:11.21\n",
      "Q:10.66\n",
      "Q:11.33\n",
      "Q:10.68\n",
      "Q:10.51\n",
      "Q:10.96\n",
      "Q:11.68\n",
      "Q:13.41\n",
      "Q:11.62\n",
      "Q:12.17\n",
      "Q:11.52\n",
      "Q:10.84\n",
      "Q:9.74\n",
      "Q:9.60\n",
      "Q:9.83\n",
      "Q:10.84\n",
      "Q:11.56\n",
      "Q:11.43\n",
      "Q:11.36\n",
      "Q:11.55\n",
      "Q:11.78\n",
      "Q:11.74\n",
      "Q:11.71\n",
      "Q:12.25\n",
      "Q:11.00\n",
      "Q:9.91\n",
      "Q:8.89\n",
      "Q:9.55\n",
      "Q:9.76\n",
      "Q:10.67\n",
      "Q:10.27\n",
      "Q:10.58\n",
      "Q:11.77\n",
      "Q:12.06\n",
      "Q:12.33\n",
      "Q:12.75\n",
      "Q:12.35\n",
      "Q:13.40\n",
      "Q:12.94\n",
      "Q:13.71\n",
      "Q:14.68\n",
      "Q:13.37\n",
      "Q:13.22\n",
      "Q:16.46\n",
      "Q:15.32\n",
      "Q:15.10\n",
      "Q:14.24\n",
      "Q:13.02\n",
      "Q:12.80\n",
      "Q:12.14\n",
      "Q:11.38\n",
      "Q:12.01\n",
      "Q:11.88\n",
      "Q:12.01\n",
      "Q:12.18\n",
      "Q:12.07\n",
      "Q:12.19\n",
      "Q:12.31\n",
      "Q:11.79\n",
      "Q:12.74\n",
      "Q:11.96\n",
      "Q:11.80\n",
      "Q:12.27\n",
      "Q:11.54\n",
      "Q:12.36\n",
      "Q:11.82\n",
      "Q:11.30\n",
      "Q:12.27\n",
      "Q:11.13\n",
      "Q:11.98\n",
      "Q:11.59\n",
      "Q:11.08\n",
      "Q:12.04\n",
      "Q:10.89\n",
      "Q:11.59\n",
      "Q:11.16\n",
      "Q:11.02\n",
      "Q:11.46\n",
      "Q:10.83\n",
      "Q:11.13\n",
      "Q:10.68\n",
      "Q:10.76\n",
      "Q:10.85\n",
      "Q:10.47\n",
      "Q:10.83\n",
      "Q:10.86\n",
      "Q:10.80\n",
      "Q:11.27\n",
      "Q:10.75\n",
      "Q:10.83\n",
      "Q:10.96\n",
      "Q:10.86\n",
      "Q:11.24\n",
      "Q:10.94\n",
      "Q:10.94\n",
      "Q:11.11\n",
      "Q:11.55\n",
      "Q:11.54\n",
      "Q:11.15\n",
      "Q:11.39\n",
      "Q:11.51\n",
      "Q:11.73\n",
      "Q:9.18\n",
      "Q:9.41\n",
      "Q:9.16\n",
      "Q:9.90\n",
      "Q:9.87\n",
      "Q:9.98\n",
      "Q:10.76\n",
      "Q:9.80\n",
      "Q:10.04\n",
      "Q:10.30\n",
      "Q:9.38\n",
      "Q:9.24\n",
      "Q:10.27\n",
      "Q:8.87\n",
      "Q:9.19\n",
      "Q:8.96\n",
      "Q:8.48\n",
      "Q:9.36\n",
      "Q:8.66\n",
      "Q:8.13\n",
      "Q:9.45\n",
      "Q:10.56\n",
      "Q:11.21\n",
      "Q:11.24\n",
      "Q:11.16\n",
      "Q:11.22\n",
      "Q:11.48\n",
      "Q:12.37\n",
      "Q:12.37\n",
      "Q:11.11\n",
      "Q:12.10\n",
      "Q:10.92\n",
      "Q:9.88\n",
      "Q:10.19\n",
      "Q:11.62\n",
      "Q:9.80\n",
      "Q:9.92\n",
      "Q:11.00\n",
      "Q:11.81\n",
      "Q:11.62\n",
      "Q:11.69\n",
      "Q:11.57\n",
      "Q:11.56\n",
      "Q:11.52\n",
      "Q:11.58\n",
      "Q:12.46\n",
      "Q:12.15\n",
      "Q:11.64\n",
      "Q:10.60\n",
      "Q:11.64\n",
      "Q:11.10\n",
      "Q:10.65\n",
      "Q:10.61\n",
      "Q:11.53\n",
      "Q:11.99\n",
      "Q:11.72\n",
      "Q:11.95\n",
      "Q:11.85\n",
      "Q:11.62\n",
      "Q:12.07\n",
      "Q:11.73\n",
      "Q:12.13\n",
      "Q:11.82\n",
      "Q:12.14\n",
      "Q:11.88\n",
      "Q:12.24\n",
      "Q:11.90\n",
      "Q:12.04\n",
      "Q:11.99\n",
      "Q:12.23\n",
      "Q:12.46\n",
      "Q:11.47\n",
      "Q:11.50\n",
      "Q:11.18\n",
      "Q:11.41\n",
      "Q:11.32\n",
      "Q:11.78\n",
      "Q:11.99\n",
      "Q:12.38\n",
      "Q:12.91\n",
      "Q:12.49\n",
      "Q:12.93\n",
      "Q:12.66\n",
      "Q:14.46\n",
      "Q:13.92\n",
      "Q:13.23\n",
      "Q:17.44\n",
      "Q:17.08\n",
      "Q:15.83\n",
      "Q:15.83\n",
      "Q:13.86\n",
      "Q:12.59\n",
      "Q:12.83\n",
      "Q:12.14\n",
      "Q:11.38\n",
      "Q:10.94\n",
      "Q:11.10\n",
      "Q:11.71\n",
      "Q:11.26\n",
      "Q:11.70\n",
      "Q:11.41\n",
      "Q:11.48\n",
      "Q:12.02\n",
      "Q:11.37\n",
      "Q:11.52\n",
      "Q:12.06\n",
      "Q:11.27\n",
      "Q:10.65\n",
      "Q:10.39\n",
      "Q:10.22\n",
      "Q:10.65\n",
      "Q:10.43\n",
      "Q:10.01\n",
      "Q:11.09\n",
      "Q:10.25\n",
      "Q:10.94\n",
      "Q:10.31\n",
      "Q:10.70\n",
      "Q:11.52\n",
      "Q:11.08\n",
      "Q:10.65\n",
      "Q:10.80\n",
      "Q:10.54\n",
      "Q:10.62\n",
      "Q:10.89\n",
      "Q:10.66\n",
      "Q:10.88\n",
      "Q:11.22\n",
      "Q:11.68\n",
      "Q:12.04\n",
      "Q:12.45\n",
      "Q:13.83\n",
      "Q:13.01\n",
      "Q:14.10\n",
      "Q:13.52\n",
      "Q:13.54\n",
      "Q:13.35\n",
      "Q:13.04\n",
      "Q:12.72\n",
      "Q:12.88\n",
      "Q:13.20\n",
      "Q:12.79\n",
      "Q:13.22\n",
      "Q:13.15\n",
      "Q:13.06\n",
      "Q:13.27\n",
      "Q:13.13\n",
      "Q:13.14\n",
      "Q:13.63\n",
      "Q:12.76\n",
      "Q:13.10\n",
      "Q:13.09\n",
      "Q:12.98\n",
      "Q:13.04\n",
      "Q:13.05\n",
      "Q:12.71\n",
      "Q:13.24\n",
      "Q:13.03\n",
      "Q:13.38\n",
      "Q:12.75\n",
      "Q:12.97\n",
      "Q:12.63\n",
      "Q:12.72\n",
      "Q:12.28\n",
      "Q:12.50\n",
      "Q:12.01\n",
      "Q:12.29\n",
      "Q:11.95\n",
      "Q:12.03\n",
      "Q:11.96\n",
      "Q:11.91\n",
      "Q:12.94\n",
      "Q:13.88\n",
      "Q:14.39\n",
      "Q:14.19\n",
      "Q:14.86\n",
      "Q:14.09\n",
      "Q:14.86\n",
      "Q:14.39\n",
      "Q:14.07\n",
      "Q:14.20\n",
      "Q:14.20\n",
      "Q:13.73\n",
      "Q:14.10\n",
      "Q:14.09\n",
      "Q:13.70\n",
      "Q:12.89\n",
      "Q:13.72\n",
      "Q:12.28\n",
      "Q:12.17\n",
      "Q:12.17\n",
      "Q:11.76\n",
      "Q:12.07\n",
      "Q:11.65\n",
      "Q:12.86\n",
      "Q:14.20\n",
      "Q:15.17\n",
      "Q:13.81\n",
      "Q:14.12\n",
      "Q:14.36\n",
      "Q:13.82\n",
      "Q:13.99\n",
      "Q:14.38\n",
      "Q:14.40\n",
      "Q:13.95\n",
      "Q:13.95\n",
      "Q:14.28\n",
      "Q:14.11\n",
      "Q:14.40\n",
      "Q:14.24\n",
      "Q:14.32\n",
      "Q:14.17\n",
      "Q:14.07\n",
      "Q:13.89\n",
      "Q:12.92\n",
      "Q:12.75\n",
      "Q:12.32\n",
      "Q:11.90\n",
      "Q:12.13\n",
      "Q:11.63\n",
      "Q:12.50\n",
      "Q:12.64\n",
      "Q:13.06\n",
      "Q:14.06\n",
      "Q:14.48\n",
      "Q:14.42\n",
      "Q:14.57\n",
      "Q:14.66\n",
      "Q:14.59\n",
      "Q:14.80\n",
      "Q:14.92\n",
      "Q:14.52\n",
      "Q:15.05\n",
      "Q:14.65\n",
      "Q:14.26\n",
      "Q:14.10\n",
      "Q:13.94\n",
      "Q:13.00\n",
      "Q:13.37\n",
      "Q:12.60\n",
      "Q:12.68\n",
      "Q:11.95\n",
      "Q:12.46\n",
      "Q:11.77\n",
      "Q:11.82\n",
      "Q:12.32\n",
      "Q:11.72\n",
      "Q:11.68\n",
      "Q:11.75\n",
      "Q:11.66\n",
      "Q:11.75\n",
      "Q:11.65\n",
      "Q:11.67\n",
      "Q:11.74\n",
      "Q:11.29\n",
      "Q:11.56\n",
      "Q:10.76\n",
      "Q:11.48\n",
      "Q:10.70\n",
      "Q:11.01\n",
      "Q:10.78\n",
      "Q:10.62\n",
      "Q:10.81\n",
      "Q:10.25\n",
      "Q:11.16\n",
      "Q:10.47\n",
      "Q:10.41\n",
      "Q:10.69\n",
      "Q:10.36\n",
      "Q:10.78\n",
      "Q:10.32\n",
      "Q:10.28\n",
      "Q:11.19\n",
      "Q:10.26\n",
      "Q:10.36\n",
      "Q:10.24\n",
      "Q:10.21\n",
      "Q:10.24\n",
      "Q:9.86\n",
      "Q:10.32\n",
      "Q:9.95\n",
      "Q:10.27\n",
      "Q:9.99\n",
      "Q:9.82\n",
      "Q:9.87\n",
      "Q:9.60\n",
      "Q:10.05\n",
      "Q:9.84\n",
      "Q:9.74\n",
      "Q:9.78\n",
      "Q:9.43\n",
      "Q:10.22\n",
      "Q:9.75\n",
      "Q:9.90\n",
      "Q:10.02\n",
      "Q:9.48\n",
      "Q:10.37\n",
      "Q:9.67\n",
      "Q:10.20\n",
      "Q:10.19\n",
      "Q:9.67\n",
      "Q:10.29\n",
      "Q:9.43\n",
      "Q:10.15\n",
      "Q:9.97\n",
      "Q:9.47\n",
      "Q:10.40\n",
      "Q:9.59\n",
      "Q:9.96\n",
      "Q:9.97\n",
      "Q:9.42\n",
      "Q:10.59\n",
      "Q:9.99\n",
      "Q:10.60\n",
      "Q:10.24\n",
      "Q:10.45\n",
      "Q:11.39\n",
      "Q:10.78\n",
      "Q:10.55\n",
      "Q:10.59\n",
      "Q:11.01\n",
      "Q:10.66\n",
      "Q:11.30\n",
      "Q:10.76\n",
      "Q:11.47\n",
      "Q:10.80\n",
      "Q:11.05\n",
      "Q:10.10\n",
      "Q:11.04\n",
      "Q:11.73\n",
      "Q:10.84\n",
      "Q:11.61\n",
      "Q:10.35\n",
      "Q:11.15\n",
      "Q:9.79\n",
      "Q:10.83\n",
      "Q:12.05\n",
      "Q:10.36\n",
      "Q:11.49\n",
      "Q:9.92\n",
      "Q:10.65\n",
      "Q:9.30\n",
      "Q:9.49\n",
      "Q:8.38\n",
      "Q:8.38\n",
      "Q:7.07\n",
      "Q:7.05\n",
      "Q:4.82\n",
      "Q:5.35\n",
      "Q:7.03\n",
      "Q:3.58\n",
      "Q:5.26\n",
      "Q:0.98\n",
      "Q:2.85\n",
      "Q:-2.70\n",
      "Q:0.26\n",
      "Q:-8.04\n",
      "Q:-4.39\n",
      "Q:-15.90\n",
      "Q:-12.59\n",
      "Q:-7.94\n",
      "Q:-21.69\n",
      "Q:-15.93\n",
      "Q:-34.77\n",
      "Q:-28.45\n",
      "Q:-25.10\n",
      "Q:-40.18\n",
      "Q:-36.80\n",
      "Q:-37.80\n",
      "Q:-46.07\n",
      "Q:-48.25\n",
      "Q:-56.19\n",
      "Q:-60.43\n",
      "Q:-67.28\n",
      "Q:-73.36\n",
      "Q:-73.90\n",
      "Q:-84.75\n",
      "Q:-86.41\n",
      "Q:-96.87\n",
      "Running 485 steps, last_state:[-2.4170926  -2.3293867  -0.13521872  0.11278569]\n"
     ]
    }
   ],
   "source": [
    "agent.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
